{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPnOdOwb-SUh"
   },
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hFObPWNAeIao"
   },
   "outputs": [],
   "source": [
    "!pip install transformers==3.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pDmsaQJ0rnyg"
   },
   "outputs": [],
   "source": [
    "def mount_drive():  \n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41570,
     "status": "ok",
     "timestamp": 1658036362572,
     "user": {
      "displayName": "­한창희 / 학생 / 자유전공학부",
      "userId": "01976388879647653081"
     },
     "user_tz": -540
    },
    "id": "sMTTH0AAiV9O",
    "outputId": "a143d4d7-9152-41db-89b0-b1ed42ac39bb"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    mount_drive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Tj07ECUMfN-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import get_linear_schedule_with_warmup, BertForSequenceClassification, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07CmMoztqKZJ"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import BertModel, BertConfig\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    # BertClassifier는 초대용량 AI 모델인 BertModel을 파인튜닝한 모델이다.\n",
    "    # 구체적으로, BertModel 위에 linear한 층 하나를 추가해 놓은 모델이다.\n",
    "    # Bert -> Linear -> Softmax의 순서로 연산이 이루어진다.\n",
    "    # 우리가 Loss를 구하는데 사용되는 CrossEntropyLoss의 경우, softmax를 자동으로 포함하여 연산해준다.\n",
    "    # 고로, BertClassifier 클래스 내의 forward 함수에, softmax는 포함시키지 않는다.\n",
    "    # 단, 학습의 용도가 아니라 테스트나 실제 시연을 할 때는 forward 결과물에 softmax를 적용시켜줘야 한다.\n",
    "\n",
    "    def __init__(self, num_labels=7):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        model_config = BertConfig.from_pretrained('monologg/kobert')\n",
    "        \n",
    "        # model_config와 from_pretrained 함수를 활용해서 bert 모델을 선언해주자.\n",
    "        # self.bert = ~~ 꼴로 작성해주자.\n",
    "        ## 여기에 코드 작성\n",
    "\n",
    "        # nn 라이브러리를 활용해서 linear 모델을 선언해주자.\n",
    "        # self.linear = ~~ 꼴로 작성해주자.\n",
    "        # bert의 경우, 768차원 벡터를 산출해준다.\n",
    "        # linear는 768차원의 값을 입력받아 7차원의 값을 출력해주어야 한다.\n",
    "        ## 여기에 코드 작성\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # 위에서 생성한 self.bert, self.linear 두 개의 layer를 활용하면 된다.\n",
    "        # bert -> linear를 적용해준 다음, 출력값을 리턴해주자.\n",
    "        # Hint: 만약 __init__에서 self.bert라는 bert layer를 생성하였고 이를 forward 함수에서 활용하려면, 생성한 bert layer의 forward 함수를 활용하면 될 것이다.\n",
    "        #       bert layer의 forward 함수는 input_ids와 attention_mask를 변수로 입력받아야 한다. 이에 따라, self.bert(input_ids, attention_mask)와 같은 방식으로 코드를 작성하면 될 것이다.\n",
    "        \n",
    "        ## 여기에 코드 작성\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GXFoQEm9-NUJ"
   },
   "outputs": [],
   "source": [
    "# 분류를 위한 BERT 모델 생성\n",
    "# BertModel을 초기화하는 역할\n",
    "def BertModelInitialization():\n",
    "    PATH = \"/content/gdrive/MyDrive/NLP/model.pt\"\n",
    "    \n",
    "    # BertModel은 다양한 작업을 진행할 수 있도록 해주는 여러 인터페이스들을 제공한다.\n",
    "    # 그 중, 본 중간 미션의 task에 가장 적합한 인터페이스를 찾아보자.\n",
    "    # Hint 1 : HuggingFace 홈페이지에 Bert를 검색해서 찾아보자.\n",
    "    # URL : https://huggingface.co/docs/transformers/main/en/index\n",
    "    # Hint 2 : 본 중간 미션이, 영화리뷰를 긍정과 부정의 두 가지 감정으로 분류해내는 작업임을 고려해보자.\n",
    "\n",
    "    model = BertClassifier()\n",
    "\n",
    "    # 생성한 모델을 특정 PATH에 저장\n",
    "    torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q8oypFVGdt2X"
   },
   "outputs": [],
   "source": [
    "def get_model(device, cuda_available):\n",
    "    PATH = \"model.pt\"\n",
    "\n",
    "    model = BertClassifier()\n",
    "\n",
    "    if cuda_available:\n",
    "        # PATH에 저장된 모델 불러옴\n",
    "        model.load_state_dict(torch.load(PATH), strict=False)\n",
    "\n",
    "        # 불러온 모델을 device에 등록\n",
    "        model = model.to(device)\n",
    "    else:\n",
    "        # PATH에 저장된 모델을 불러오기 및 불러온 모델을 device에 등록\n",
    "        model.load_state_dict(torch.load(PATH, map_location=device), strict=False)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUf6MZ3zplat"
   },
   "source": [
    "**신경망 성능 향상을 위한 다양한 툴**\n",
    "\n",
    "\n",
    "\n",
    "> 최종미션을 위한 이론문서를 확인해보자.\n",
    "\n",
    "\n",
    "- 여러분은 옵티마이저, 에포크 수, 스케줄러 종류를 본 task에 가장 적합한 것으로 선택하여 바꿀 수 있다.\n",
    "- 정답은 없다. 최적의 모델을 선택하여 신경망의 성능을 높여보자!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BcFCBaNkXxrv"
   },
   "outputs": [],
   "source": [
    "def get_model_with_params(num_data, device, cuda_available):\n",
    "    model = get_model(device, cuda_available)\n",
    "\n",
    "    # 옵티마이저 설정\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                      lr = 1e-5, # 학습률\n",
    "                      eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값,\n",
    "                    )\n",
    "    # 전체 데이터가 총 몇 번 학습되는지\n",
    "    epochs = 3\n",
    "\n",
    "    # 총 훈련 스텝\n",
    "    total_steps = num_data * epochs\n",
    "\n",
    "    # 학습이 이루어짐에 따라 learning_rate을 감소시키기 위한 스케줄러\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps = 0,\n",
    "                                                num_training_steps = total_steps)\n",
    "  \n",
    "    # nn 라이브러리에서 crossentropy 형식으로 loss를 계산하는 모델을 찾아서 손실함수로 설정해주자.\n",
    "    # criterion = ~~ 꼴로 작성해주자.\n",
    "    ## 여기에 코드 작성\n",
    "\n",
    "    \n",
    "    return model, optimizer, scheduler, epochs, criterion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihB4OGzxscad"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    %cd /content/gdrive/MyDrive/NLP\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    BertModelInitialization()\n",
    "    print(get_model_with_params(34388, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KvhcU7f8Vdxy"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "      main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "nlp_model_final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
